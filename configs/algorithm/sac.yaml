name: SAC
params:
  batch_size: 256
  gamma: 0.99
  learning_rate: 0.0003
  learning_starts: 100 # warm up steps
  implementation: sb3  # Options: "custom", "sb3"
  policy: "MlpPolicy"
  gradient_steps: 1 # gradient_steps determines the number of parameter updates performed during each training iteration, while train_freq controls the frequency of training updates
  net_arch: {pi: [256, 256], qf: [256, 256]}
  activation_fn: "ReLU"
  tau: 0.005
  buffer_size: 1000000
  train_freq: 1 # takes tuple for (int, str) for (n_updates, "step" or "episode")
  use_sde: False
  sde_sample_freq: -1 # -1 = only sample at rollout beginning 
  log_std_init: -3
  action_noise: "" # Choose between pink or ""
  replay_buffer_class: "" # Choose between ERE, if "" internally defaults to SB3 ReplayBuffer class
  replay_buffer_kwargs:
    eta0: 0.996 
    etaT: 1
    cmin: 5000
    total_timesteps: ${mode.total_timesteps}
    use_per: false
    alpha: 0.6 # prioritization exponent
    beta0: 0.4 # importance sampling exponent at time t=0
    betaT: 1 # importance sampling exponent at time t=T 
  use_cepo: false
  cepo_kwargs:
    ce_N: 100 # sample number N
    ce_ne: 5 # # number of elites
    ce_size:  0.33 # elite fraction
    ce_t: 10 # number of iterations
    
